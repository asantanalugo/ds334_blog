[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Blog Post #2",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/alexasantana/Documents/SLU/Semester 6/Data 336/ds334_blog\n\nairlines_df &lt;- read_csv(here(\"week19_airline_safety.csv\"))\n\nNew names:\nRows: 336 Columns: 6\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): airline, year_range, type_of_event dbl (3): ...1, avail_seat_km_per_week,\nn_events\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nairlines_df |&gt; group_by(type_of_event) |&gt; \n  arrange(desc(n_events))\n\n# A tibble: 336 × 6\n# Groups:   type_of_event [3]\n    ...1 airline        avail_seat_km_per_week year_range type_of_event n_events\n   &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1   315 Malaysia Airl…             1039171244 00_14      fatalities         537\n 2   129 China Airlines              813216487 85_99      fatalities         535\n 3   141 Japan Airlines             1574217531 85_99      fatalities         520\n 4   144 Korean Air                 1734522605 85_99      fatalities         425\n 5   292 American*                  5228357340 00_14      fatalities         416\n 6   132 Delta / North…             6525658894 85_99      fatalities         407\n 7   286 Air France                 3004002661 00_14      fatalities         337\n 8   119 Air India*                  869253552 85_99      fatalities         329\n 9   126 Avianca                     396922563 85_99      fatalities         323\n10   164 United / Cont…             7139291291 85_99      fatalities         319\n# ℹ 326 more rows\n\nairlines_df |&gt; group_by(type_of_event) |&gt; \n  arrange(desc(avail_seat_km_per_week)) |&gt; \n  print(n = 40)\n\n# A tibble: 336 × 6\n# Groups:   type_of_event [3]\n    ...1 airline        avail_seat_km_per_week year_range type_of_event n_events\n   &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1    52 United / Cont…             7139291291 85_99      incidents           19\n 2   108 United / Cont…             7139291291 85_99      fatal_accide…        8\n 3   164 United / Cont…             7139291291 85_99      fatalities         319\n 4   220 United / Cont…             7139291291 00_14      incidents           14\n 5   276 United / Cont…             7139291291 00_14      fatal_accide…        2\n 6   332 United / Cont…             7139291291 00_14      fatalities         109\n 7    20 Delta / North…             6525658894 85_99      incidents           24\n 8    76 Delta / North…             6525658894 85_99      fatal_accide…       12\n 9   132 Delta / North…             6525658894 85_99      fatalities         407\n10   188 Delta / North…             6525658894 00_14      incidents           24\n11   244 Delta / North…             6525658894 00_14      fatal_accide…        2\n12   300 Delta / North…             6525658894 00_14      fatalities          51\n13    12 American*                  5228357340 85_99      incidents           21\n14    68 American*                  5228357340 85_99      fatal_accide…        5\n15   124 American*                  5228357340 85_99      fatalities         101\n16   180 American*                  5228357340 00_14      incidents           17\n17   236 American*                  5228357340 00_14      fatal_accide…        3\n18   292 American*                  5228357340 00_14      fatalities         416\n19    34 Lufthansa*                 3426529504 85_99      incidents            6\n20    90 Lufthansa*                 3426529504 85_99      fatal_accide…        1\n21   146 Lufthansa*                 3426529504 85_99      fatalities           2\n22   202 Lufthansa*                 3426529504 00_14      incidents            3\n23   258 Lufthansa*                 3426529504 00_14      fatal_accide…        0\n24   314 Lufthansa*                 3426529504 00_14      fatalities           0\n25    44 Southwest Air…             3276525770 85_99      incidents            1\n26   100 Southwest Air…             3276525770 85_99      fatal_accide…        0\n27   156 Southwest Air…             3276525770 85_99      fatalities           0\n28   212 Southwest Air…             3276525770 00_14      incidents            8\n29   268 Southwest Air…             3276525770 00_14      fatal_accide…        0\n30   324 Southwest Air…             3276525770 00_14      fatalities           0\n31    15 British Airwa…             3179760952 85_99      incidents            4\n32    71 British Airwa…             3179760952 85_99      fatal_accide…        0\n33   127 British Airwa…             3179760952 85_99      fatalities           0\n34   183 British Airwa…             3179760952 00_14      incidents            6\n35   239 British Airwa…             3179760952 00_14      fatal_accide…        0\n36   295 British Airwa…             3179760952 00_14      fatalities           0\n37     6 Air France                 3004002661 85_99      incidents           14\n38    62 Air France                 3004002661 85_99      fatal_accide…        4\n39   118 Air France                 3004002661 85_99      fatalities          79\n40   174 Air France                 3004002661 00_14      incidents            6\n# ℹ 296 more rows\n\n## take n_events to avail_seat_km_per_week ratio? something like that?? that can help see relationship between how much do planes travel vs. how much many events they've had"
  },
  {
    "objectID": "posts/welcome/index.html#blog-post-2",
    "href": "posts/welcome/index.html#blog-post-2",
    "title": "Blog Post #2",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/alexasantana/Documents/SLU/Semester 6/Data 336/ds334_blog\n\nairlines_df &lt;- read_csv(here(\"week19_airline_safety.csv\"))\n\nNew names:\nRows: 336 Columns: 6\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): airline, year_range, type_of_event dbl (3): ...1, avail_seat_km_per_week,\nn_events\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nairlines_df |&gt; group_by(type_of_event) |&gt; \n  arrange(desc(n_events))\n\n# A tibble: 336 × 6\n# Groups:   type_of_event [3]\n    ...1 airline        avail_seat_km_per_week year_range type_of_event n_events\n   &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1   315 Malaysia Airl…             1039171244 00_14      fatalities         537\n 2   129 China Airlines              813216487 85_99      fatalities         535\n 3   141 Japan Airlines             1574217531 85_99      fatalities         520\n 4   144 Korean Air                 1734522605 85_99      fatalities         425\n 5   292 American*                  5228357340 00_14      fatalities         416\n 6   132 Delta / North…             6525658894 85_99      fatalities         407\n 7   286 Air France                 3004002661 00_14      fatalities         337\n 8   119 Air India*                  869253552 85_99      fatalities         329\n 9   126 Avianca                     396922563 85_99      fatalities         323\n10   164 United / Cont…             7139291291 85_99      fatalities         319\n# ℹ 326 more rows\n\nairlines_df |&gt; group_by(type_of_event) |&gt; \n  arrange(desc(avail_seat_km_per_week)) |&gt; \n  print(n = 40)\n\n# A tibble: 336 × 6\n# Groups:   type_of_event [3]\n    ...1 airline        avail_seat_km_per_week year_range type_of_event n_events\n   &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1    52 United / Cont…             7139291291 85_99      incidents           19\n 2   108 United / Cont…             7139291291 85_99      fatal_accide…        8\n 3   164 United / Cont…             7139291291 85_99      fatalities         319\n 4   220 United / Cont…             7139291291 00_14      incidents           14\n 5   276 United / Cont…             7139291291 00_14      fatal_accide…        2\n 6   332 United / Cont…             7139291291 00_14      fatalities         109\n 7    20 Delta / North…             6525658894 85_99      incidents           24\n 8    76 Delta / North…             6525658894 85_99      fatal_accide…       12\n 9   132 Delta / North…             6525658894 85_99      fatalities         407\n10   188 Delta / North…             6525658894 00_14      incidents           24\n11   244 Delta / North…             6525658894 00_14      fatal_accide…        2\n12   300 Delta / North…             6525658894 00_14      fatalities          51\n13    12 American*                  5228357340 85_99      incidents           21\n14    68 American*                  5228357340 85_99      fatal_accide…        5\n15   124 American*                  5228357340 85_99      fatalities         101\n16   180 American*                  5228357340 00_14      incidents           17\n17   236 American*                  5228357340 00_14      fatal_accide…        3\n18   292 American*                  5228357340 00_14      fatalities         416\n19    34 Lufthansa*                 3426529504 85_99      incidents            6\n20    90 Lufthansa*                 3426529504 85_99      fatal_accide…        1\n21   146 Lufthansa*                 3426529504 85_99      fatalities           2\n22   202 Lufthansa*                 3426529504 00_14      incidents            3\n23   258 Lufthansa*                 3426529504 00_14      fatal_accide…        0\n24   314 Lufthansa*                 3426529504 00_14      fatalities           0\n25    44 Southwest Air…             3276525770 85_99      incidents            1\n26   100 Southwest Air…             3276525770 85_99      fatal_accide…        0\n27   156 Southwest Air…             3276525770 85_99      fatalities           0\n28   212 Southwest Air…             3276525770 00_14      incidents            8\n29   268 Southwest Air…             3276525770 00_14      fatal_accide…        0\n30   324 Southwest Air…             3276525770 00_14      fatalities           0\n31    15 British Airwa…             3179760952 85_99      incidents            4\n32    71 British Airwa…             3179760952 85_99      fatal_accide…        0\n33   127 British Airwa…             3179760952 85_99      fatalities           0\n34   183 British Airwa…             3179760952 00_14      incidents            6\n35   239 British Airwa…             3179760952 00_14      fatal_accide…        0\n36   295 British Airwa…             3179760952 00_14      fatalities           0\n37     6 Air France                 3004002661 85_99      incidents           14\n38    62 Air France                 3004002661 85_99      fatal_accide…        4\n39   118 Air France                 3004002661 85_99      fatalities          79\n40   174 Air France                 3004002661 00_14      incidents            6\n# ℹ 296 more rows\n\n## take n_events to avail_seat_km_per_week ratio? something like that?? that can help see relationship between how much do planes travel vs. how much many events they've had"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Blog Post #1",
    "section": "",
    "text": "Taken form the Tidy Tuesday GitHub (https://github.com/rfordatascience/tidytuesday) repository (Week 30 of 2025), the selected dataset looks at TV & Movie streaming data from Netflix. According to Tidy Tuesday, Netflix releases Engagement Reports which breakdown the amount of hours users have spent watching both Movies & TV Shows. In this blog post we will look at trying to visualize what are the most watched movies and TV shows in each report, and try to identify if the most watched shows / movies are american, or international!\n\n## loading in packages: \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## loading in the data: \ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 30)\n\n---- Compiling #TidyTuesday Information for 2025-07-29 ----\n--- There are 2 files available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 2: \"movies.csv\"\n  2 of 2: \"shows.csv\"\n\nmovies &lt;- \n  readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv')\n\nRows: 36121 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): source, report, title, available_globally, runtime\ndbl  (2): hours_viewed, views\ndate (1): release_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nshows &lt;- \n  readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv')\n\nRows: 27803 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): source, report, title, available_globally, runtime\ndbl  (2): hours_viewed, views\ndate (1): release_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n## Looking at movies & TV Shows individually & cleaning the data: \n\n## movies: \nmovies_df &lt;- \n  movies |&gt; select(- source) |&gt; group_by(title) |&gt; \n  arrange(desc(hours_viewed)) |&gt; ungroup(title) |&gt; \n  mutate(category = \"movies\")\n\nmovies_top_10 &lt;- \n  movies_df |&gt; slice(1:10) |&gt; \n  mutate(title = recode(title, \n                        \"Society of the Snow // La sociedad de la nieve\" = \n                          \"Society of the Snow\")) |&gt; \n  mutate(title = fct_reorder(title, hours_viewed))\n  \n\n\n## shows: \nshows_df &lt;- \n  shows |&gt; select(- source) |&gt; group_by(title) |&gt; \n  arrange(desc(hours_viewed)) |&gt; ungroup(title) |&gt; \n  mutate(category = \"shows\")\n\nshows_top_10 &lt;- \n  shows_df |&gt; slice(1:10) |&gt; \n  mutate(title = recode(title, \n                        \"Squid Game: Season 2 // 오징어 게임: 시즌 2\" = \"Squid Game S2\", \n                        \"Bridgerton: Season 3\" = \"Bridgerton S3\", \n                        \"Fool Me Once: Limited Series\" = \"Fool Me Once\", \n                        \"Queen of Tears: Limited Series // 눈물의 여왕: 리미티드 시리즈\" = \n                          \"Queen of Tears\", \n                        \"King the Land: Limited Series // 킹더랜드: 리미티드 시리즈\" = \n                          \"King of the Land\", \n                        \"Squid Game: Season 2 // 오징어 게임: 시즌 2\" = \"Squid Game S2\", \n                        \"When Life Gives You Tangerines: Limited Series // 폭싹 속았수다: 리미티드 시리즈\" = \"When Life Gives You Tang\", \n                        \"Adolescence: Limited Series\" = \"Adolescence\", \n                        \"Monsters: The Lyle and Erik Menendez Story\" = \"Monsters\", \n                        \"ONE PIECE: Season 1\" = \"One Piece\")) |&gt; \n  mutate(title = fct_reorder(title, hours_viewed))\n  \n\n\n## Visualizing at most populat shows & TV shows by hours viewed\n\n## for movies\nggplot(data = movies_top_10, aes(x = hours_viewed, y = title)) + \n  geom_segment(aes(yend = title, x = 0, xend = hours_viewed), \n               show.legend = FALSE) +\n  geom_point(color = \"dodgerblue1\", size = 1.5, show.legend = FALSE) + \n  theme_minimal() + \n  scale_size_manual(values = c(1, 0.5)) + \n  labs(title = \"Most Watched Movies by Hours Viewed in Half a Year\", \n       x = \"Movie Title\", y = \"Hours Viewed\")\n\n\n\n\n\n\n\n## for shows: \nggplot(data = shows_top_10, aes(x = hours_viewed, y = title)) + \n  geom_segment(aes(yend = title, x = 0, xend = hours_viewed), show.legend = FALSE) + \n  geom_point(color = \"dodgerblue1\", show.legend = FALSE) + \n  theme_minimal() + \n  scale_size_manual(values = c(1, 0.5)) + \n  labs(title = \"Most Watched Shows by Hours Viewed in Half a Year\", \n       x = \"Show Title\", y = \"Hours Viewed\")\n\n\n\n\n\n\n\n\nFor movies: Regarding movies, the most watched movie was Back in Action from January to June 2025, with about 313 million hours viewed. The next is Leave the World Behind with about 286 million hours viewed. What I find the most interesting of these 10 most watched movies, is that they are all originally produced movies by Netflix, and most of them are american movies.\nFor shows: The most watched show is Squid Game, with more than 840 million hours views from January to June 2025, nonetheless, it also re-appears in the list, as the number 6th most viewed from July to December 2024 with 216 million hours views. It is important to note, that this is the same season 2, which makes this show and season the most viewed by far! Additionally, Bridgerton Season 3 and Fool me once (limited series) are next in the ranking. Answering out question of interest, regarding shows, None of these shows are american, they are all international with the exception of Monsters (Menendez Brother’s story).\nConclusion: There is a big difference in consumption between TV Shows and Movies on Netflix. I would infer that Netflix has a stronger marketing strategy for their own produced movies, and are widely consumed, as they are advertised strongly by the platform.\nConnection to class: I used the lolly plot, as this is more effective than using a bar plot for these types of quantitative & categorical variables. We are starting the scale at zero, so none of our results are misleading anyone, and we can see the actual overall trend of the views."
  },
  {
    "objectID": "posts/post-with-code/index.html#netflix-reports-blog-post-1",
    "href": "posts/post-with-code/index.html#netflix-reports-blog-post-1",
    "title": "Blog Post #1",
    "section": "",
    "text": "Taken form the Tidy Tuesday GitHub (https://github.com/rfordatascience/tidytuesday) repository (Week 30 of 2025), the selected dataset looks at TV & Movie streaming data from Netflix. According to Tidy Tuesday, Netflix releases Engagement Reports which breakdown the amount of hours users have spent watching both Movies & TV Shows. In this blog post we will look at trying to visualize what are the most watched movies and TV shows in each report, and try to identify if the most watched shows / movies are american, or international!\n\n## loading in packages: \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## loading in the data: \ntuesdata &lt;- tidytuesdayR::tt_load(2025, week = 30)\n\n---- Compiling #TidyTuesday Information for 2025-07-29 ----\n--- There are 2 files available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 2: \"movies.csv\"\n  2 of 2: \"shows.csv\"\n\nmovies &lt;- \n  readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/movies.csv')\n\nRows: 36121 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): source, report, title, available_globally, runtime\ndbl  (2): hours_viewed, views\ndate (1): release_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nshows &lt;- \n  readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-29/shows.csv')\n\nRows: 27803 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): source, report, title, available_globally, runtime\ndbl  (2): hours_viewed, views\ndate (1): release_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n## Looking at movies & TV Shows individually & cleaning the data: \n\n## movies: \nmovies_df &lt;- \n  movies |&gt; select(- source) |&gt; group_by(title) |&gt; \n  arrange(desc(hours_viewed)) |&gt; ungroup(title) |&gt; \n  mutate(category = \"movies\")\n\nmovies_top_10 &lt;- \n  movies_df |&gt; slice(1:10) |&gt; \n  mutate(title = recode(title, \n                        \"Society of the Snow // La sociedad de la nieve\" = \n                          \"Society of the Snow\")) |&gt; \n  mutate(title = fct_reorder(title, hours_viewed))\n  \n\n\n## shows: \nshows_df &lt;- \n  shows |&gt; select(- source) |&gt; group_by(title) |&gt; \n  arrange(desc(hours_viewed)) |&gt; ungroup(title) |&gt; \n  mutate(category = \"shows\")\n\nshows_top_10 &lt;- \n  shows_df |&gt; slice(1:10) |&gt; \n  mutate(title = recode(title, \n                        \"Squid Game: Season 2 // 오징어 게임: 시즌 2\" = \"Squid Game S2\", \n                        \"Bridgerton: Season 3\" = \"Bridgerton S3\", \n                        \"Fool Me Once: Limited Series\" = \"Fool Me Once\", \n                        \"Queen of Tears: Limited Series // 눈물의 여왕: 리미티드 시리즈\" = \n                          \"Queen of Tears\", \n                        \"King the Land: Limited Series // 킹더랜드: 리미티드 시리즈\" = \n                          \"King of the Land\", \n                        \"Squid Game: Season 2 // 오징어 게임: 시즌 2\" = \"Squid Game S2\", \n                        \"When Life Gives You Tangerines: Limited Series // 폭싹 속았수다: 리미티드 시리즈\" = \"When Life Gives You Tang\", \n                        \"Adolescence: Limited Series\" = \"Adolescence\", \n                        \"Monsters: The Lyle and Erik Menendez Story\" = \"Monsters\", \n                        \"ONE PIECE: Season 1\" = \"One Piece\")) |&gt; \n  mutate(title = fct_reorder(title, hours_viewed))\n  \n\n\n## Visualizing at most populat shows & TV shows by hours viewed\n\n## for movies\nggplot(data = movies_top_10, aes(x = hours_viewed, y = title)) + \n  geom_segment(aes(yend = title, x = 0, xend = hours_viewed), \n               show.legend = FALSE) +\n  geom_point(color = \"dodgerblue1\", size = 1.5, show.legend = FALSE) + \n  theme_minimal() + \n  scale_size_manual(values = c(1, 0.5)) + \n  labs(title = \"Most Watched Movies by Hours Viewed in Half a Year\", \n       x = \"Movie Title\", y = \"Hours Viewed\")\n\n\n\n\n\n\n\n## for shows: \nggplot(data = shows_top_10, aes(x = hours_viewed, y = title)) + \n  geom_segment(aes(yend = title, x = 0, xend = hours_viewed), show.legend = FALSE) + \n  geom_point(color = \"dodgerblue1\", show.legend = FALSE) + \n  theme_minimal() + \n  scale_size_manual(values = c(1, 0.5)) + \n  labs(title = \"Most Watched Shows by Hours Viewed in Half a Year\", \n       x = \"Show Title\", y = \"Hours Viewed\")\n\n\n\n\n\n\n\n\nFor movies: Regarding movies, the most watched movie was Back in Action from January to June 2025, with about 313 million hours viewed. The next is Leave the World Behind with about 286 million hours viewed. What I find the most interesting of these 10 most watched movies, is that they are all originally produced movies by Netflix, and most of them are american movies.\nFor shows: The most watched show is Squid Game, with more than 840 million hours views from January to June 2025, nonetheless, it also re-appears in the list, as the number 6th most viewed from July to December 2024 with 216 million hours views. It is important to note, that this is the same season 2, which makes this show and season the most viewed by far! Additionally, Bridgerton Season 3 and Fool me once (limited series) are next in the ranking. Answering out question of interest, regarding shows, None of these shows are american, they are all international with the exception of Monsters (Menendez Brother’s story).\nConclusion: There is a big difference in consumption between TV Shows and Movies on Netflix. I would infer that Netflix has a stronger marketing strategy for their own produced movies, and are widely consumed, as they are advertised strongly by the platform.\nConnection to class: I used the lolly plot, as this is more effective than using a bar plot for these types of quantitative & categorical variables. We are starting the scale at zero, so none of our results are misleading anyone, and we can see the actual overall trend of the views."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ds334_blog",
    "section": "",
    "text": "Blog Post #2\n\n\n\n\n\n\nnetflix\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 16, 2026\n\n\nAlexa Santana\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post #1\n\n\n\n\n\n\nnetflix\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 10, 2026\n\n\nAlexa Santana\n\n\n\n\n\n\nNo matching items"
  }
]